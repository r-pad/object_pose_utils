{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mulitview YCB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import quat_math\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import scipy.io as scio\n",
    "from functools import partial\n",
    "from object_pose_utils.utils import to_np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Image Viewing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshowCV(img, axis = False, show = True):\n",
    "    if not axis:\n",
    "        plt.axis('off')\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    if(show):\n",
    "        plt.show()\n",
    "    \n",
    "def imshow(img, axis = False, colorbar = False, show = True):\n",
    "    if not axis:\n",
    "        plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    if(colorbar):\n",
    "        plt.colorbar()\n",
    "    if(show):\n",
    "        plt.show()\n",
    "    \n",
    "def torch2Img(img, show = True):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    disp_img = to_np(img)\n",
    "    if len(disp_img.shape) == 4:\n",
    "        disp_img = disp_img[0]\n",
    "    disp_img = disp_img.transpose((1,2,0))\n",
    "    disp_img = disp_img * std + mean\n",
    "    return disp_img\n",
    "    \n",
    "def imshowTorch(img, axis = False, show = True):\n",
    "    if not axis:\n",
    "        plt.axis('off')\n",
    "    disp_img = torch2Img(img)\n",
    "    plt.imshow(disp_img.astype(np.uint8))\n",
    "    if(show):\n",
    "        plt.show()\n",
    "\n",
    "def plotImageScatter(img, choose, show = True):\n",
    "    coords = np.unravel_index(choose, img.shape[:2])    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.astype(np.uint8))    \n",
    "    plt.scatter(coords[1], coords[0], 50)\n",
    "    #plt.colorbar()\n",
    "    if(show):\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location of YCB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_root = '/ssd0/datasets/ycb/YCB_Video_Dataset'\n",
    "dataset_root = '/home/bokorn/data/ycb/debug/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Object Indices of Interest\n",
    "\n",
    "| Object Indices |[]()|[]()|\n",
    "|---|---|---|\n",
    "| __1.__ 002_master_chef_can | __8.__ 009_gelatin_box      | __15.__ 035_power_drill       |\n",
    "| __2.__ 003_cracker_box     | __9.__ 010_potted_meat_can  | __16.__ 036_wood_block        |\n",
    "| __3.__ 004_sugar_box       | __10.__ 011_banana          | __17.__ 037_scissors          |\n",
    "| __4.__ 005_tomato_soup_can | __11.__ 019_pitcher_base    | __18.__ 040_large_marker      |\n",
    "| __5.__ 006_mustard_bottle  | __12.__ 021_bleach_cleanser | __19.__ 051_large_clamp       |\n",
    "| __6.__ 007_tuna_fish_can   | __13.__ 024_bowl            | __20.__ 052_extra_large_clamp |\n",
    "| __7.__ 008_pudding_box     | __14.__ 025_mug             | __21.__ 061_foam_brick        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_list = [14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Dataset\n",
    "Modes include train, syn, grid, valid, test and can be concatinated with \"\\_\", e.g. \"train\\_syn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Output Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.datasets.pose_dataset import OutputTypes as otypes\n",
    "\n",
    "output_format = [otypes.OBJECT_LABEL,\n",
    "                 otypes.QUATERNION, \n",
    "                 otypes.IMAGE_CROPPED, \n",
    "                 otypes.DEPTH_POINTS_MASKED_AND_INDEXES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize YCB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.datasets.ycb_dataset import YcbDataset as YCBDataset\n",
    "from object_pose_utils.datasets.image_processing import ImageNormalizer\n",
    "\n",
    "ycb_dataset = YCBDataset(dataset_root, mode=mode, \n",
    "                         object_list = object_list, \n",
    "                         output_data = output_format, \n",
    "                         postprocessors = [ImageNormalizer()],\n",
    "                         image_size = [640, 480], num_points=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Image Set Parameters\n",
    "Inteval between images and max number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 4\n",
    "video_len = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize YCB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.datasets.ycb_video_dataset import YcbVideoDataset as YCBVideoDataset\n",
    "\n",
    "dataset = YCBVideoDataset(ycb_dataset, \n",
    "                          interval = interval, \n",
    "                          video_len = video_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Object If Mutiple Objects Are Avalible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avalible objects: 14\n"
     ]
    }
   ],
   "source": [
    "print('Avalible objects: {}'.format(*dataset.getObjectIds()))\n",
    "dataset.setObjectId(object_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Video Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avalible Videos: 0000\n",
      "9 image sets avaible in video 0000\n"
     ]
    }
   ],
   "source": [
    "print('Avalible Videos: {}'.format(*dataset.getVideoIds()))\n",
    "video_id = '0000'\n",
    "dataset.setVideoId(video_id)\n",
    "print('{} image sets avaible in video {}'.format(len(dataset), video_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the dataset\n",
    "\n",
    "for data, trans in dataset:\n",
    "    break\n",
    "\n",
    "# Or if you want to grab a specific index\n",
    "#idx = 0\n",
    "#data, trans = dataset.__getitem__(idx)\n",
    "#data = dataset.getData(idx)\n",
    "#trans = dataset.getCameraTransforms(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.utils.multi_view_utils import applyTransform, computeCameraTransform\n",
    "\n",
    "quats = []\n",
    "for mat, d in zip(trans, data):\n",
    "    quats.append(d[1])\n",
    "\n",
    "quats_trans = applyTransform(quats, trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resulting Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020641618571863796 -> 0.0\n",
      "0.2910394930802486 -> 0.0\n",
      "0.48652849477105786 -> 0.06277192093494893\n"
     ]
    }
   ],
   "source": [
    "from object_pose_utils.utils.pose_processing import quatAngularDiff\n",
    "q0 = to_np(data[0][1])\n",
    "for q, q_t in zip(quats, quats_trans):\n",
    "    dq = quatAngularDiff(q0, q)*180/np.pi\n",
    "    dq_t = quatAngularDiff(q0, q_t)*180/np.pi\n",
    "    print(\"{} -> {}\".format(dq, dq_t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using with DenseFusion Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 4 required positional arguments: 'img', 'x', 'choose', and 'obj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-64ad7ea45156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#estimator.cuda();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/generic_pose/bpy/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 4 required positional arguments: 'img', 'x', 'choose', and 'obj'"
     ]
    }
   ],
   "source": [
    "from dense_fusion.network import PoseNetGlobal, PoseNet\n",
    "model_checkpoint = '/home/bokorn/src/DenseFusion/trained_checkpoints/ycb/pose_model_train_split_34_0.025648579025031315.pth'\n",
    "\n",
    "estimator = PoseNet(num_points = 1000, \n",
    "                    num_obj = 21)\n",
    "estimator.load_state_dict(torch.load(model_checkpoint, map_location=lambda storage, loc: storage))\n",
    "#estimator.cuda();\n",
    "\n",
    "estimator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate rotation for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_quats = []\n",
    "for mat, d in zip(trans, data):\n",
    "    idx, quat, img, points, choose, = d\n",
    "    idx = idx - 1\n",
    "    points, choose, img, idx = Variable(points.unsqueeze(0)), \\\n",
    "                               Variable(choose.unsqueeze(0)), \\\n",
    "                               Variable(img.unsqueeze(0)), \\\n",
    "                               Variable(idx.unsqueeze(0))\n",
    "    \n",
    "    pred_r, pred_t, pred_c, emb = estimator(img, points, choose, idx)\n",
    "    pred_q = pred_r[0,torch.argmax(pred_c)][[1,2,3,0]]\n",
    "    pred_q /= pred_q.norm()\n",
    "    est_quats.append(pred_q)\n",
    "\n",
    "est_quats_trans = applyTransform(est_quats, trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resulting Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = to_np(est_quats[0])\n",
    "for q, q_t in zip(est_quats, est_quats_trans):\n",
    "    dq = quatAngularDiff(q0, to_np(q))*180/np.pi\n",
    "    dq_t = quatAngularDiff(q0, q_t)*180/np.pi\n",
    "    print(\"{} -> {}\".format(dq, dq_t))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpy_kernel",
   "language": "python",
   "name": "bpy_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
